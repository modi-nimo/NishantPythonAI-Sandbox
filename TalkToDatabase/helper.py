import json
import os
import pprint

import pandas as pd
from chromadb import Settings
from dotenv import load_dotenv
import psycopg
import uuid
import chromadb
from pydantic import BaseModel
from google import genai
from google.genai import types
from agno.agent.agent import Agent
from groq import Groq
import asyncio # Import asyncio

load_dotenv()

async def _publish_update_to_queue(agent: Agent):
    update_queue = agent.team_session_state["update_queue"]
    app_response = agent.team_session_state["application_response"]

    # Create a dictionary with the current state of the application_response
    # Ensure dataframe is converted to a serializable format
    current_state = {
        "user_question": app_response.user_question,
        "generated_sql_query": app_response.generated_sql_query,
        "explanation": app_response.explanation,
        "dataframe": app_response.dataframe.to_dict(orient="records") if app_response.dataframe is not None else None,
        "insights": app_response.insights
    }
    # Put the JSON string into the queue
    await update_queue.put(json.dumps(current_state))

#
# from openinference.instrumentation.agno import AgnoInstrumentor
# from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
# from opentelemetry.sdk import trace as trace_sdk
# from opentelemetry import trace as trace_api
# from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor
#
# endpoint = "http://127.0.0.1:6006/v1/traces"
# tracer_provider = trace_sdk.TracerProvider()
# tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))
# # Optionally, you can also print the spans to the console.
# tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
#
# trace_api.set_tracer_provider(tracer_provider=tracer_provider)
#
# # Start instrumenting agno
# AgnoInstrumentor().instrument()

"""
4. Chart And Graph Generator: That generates charts and graphs based on the data in the database. It will use the dataframe.
6. Logger to log the queries and the responses from the database.
Possible Improvements:
- ChromaDB is taking a lot of time to generate embeddings. Can we speed it up.
"""


class SQLOutput(BaseModel):
    generated_sql_query: str
    explanation: str = None

def debug_sql_query(agent: Agent, error_message:str ) -> str:
    """
        Debug a SQL query based on the error message and the sql query generated by the SQL Manager Agent.
        :param error_message: The error message that needs to be debugged.
        :param agent: Agno Agent: The agent that will generate the SQL query
        :return: str: The corrected SQL query.

        """
    # First we will clean the User Question.
    user_question = agent.team_session_state["application_response"].user_question

    # Then we will use the ChromaDB to retrieve the relevant tables and columns.
    client = chromadb.PersistentClient(path="Embeddings/", settings=Settings(anonymized_telemetry=False))
    table_collection = client.get_collection(name="table_names")
    tables_result = table_collection.query(
        query_texts=[user_question],
        n_results=3
    )

    # Then we will use the retrieved tables and columns to generate the SQL query.
    column_collection = client.get_collection(name="column_names")
    columns_result = column_collection.query(
        query_texts=[user_question],
        n_results=5
    )

    db_type = "Postgres"

    debug_prompt = f""" You are an expert SQL query debugger. 
    Your task is to understand the error message for the given SQL and fix it . The final query should be in {db_type}-compatible SQL queries.

        You will be provided with:
        1.  **User Question:** {user_question}
        2.  **Relevant Tables:** {','.join(tables_result["documents"][0])}
        3.  **Relevant Column Data :** {' \n '.join([str(single_col) for single_col in columns_result["metadatas"][0]])}
        4.  **Database Schema:** dbo
        5. **Error Message:** {error_message}
        6. **Generated SQL Query:** {agent.team_session_state["application_response"].generated_sql_query}
        ---

        **Instructions to follow:**

        * **Compatibility:** Ensure all generated SQL syntax, functions, and data types are compatible with {db_type}.
        * **Clarity and Readability:** Generate clear, well-formatted SQL queries.
        * **Accuracy:** The query must accurately address the user's question based on the provided information.
        * **Efficiency (where applicable):** Consider common query optimization techniques.
        * **Error Handling/Limitations:** If a query cannot be generated with the provided information, explain why.
        * **Common Operations:** Be prepared to handle `SELECT`, `WHERE`, `JOIN` (INNER, LEFT, RIGHT, FULL), `GROUP BY`, `ORDER BY`, `LIMIT`, `OFFSET`,
        aggregate functions (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`), subqueries, and common {db_type}-specific functions (e.g., `DATE_TRUNC`, `EXTRACT`, `COALESCE`).


    """

    client = genai.Client(api_key=os.environ["GOOGLE_API_KEY"])
    llm_response = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=debug_prompt,
        config=types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(thinking_budget=-1),
            temperature=0.2,
            response_mime_type="application/json",
            response_schema=SQLOutput
        )
    )

    output_response: SQLOutput = llm_response.parsed
    agent.team_session_state["application_response"].generated_sql_query = output_response.generated_sql_query
    agent.team_session_state["application_response"].explanation = output_response.explanation
    asyncio.run(_publish_update_to_queue(agent)) # Publish update

    return output_response.generated_sql_query

def generate_sql_query(agent: Agent) -> str:
    """
    Generates a SQL query based on the user's question.
    :param agent: Agno Agent: The agent that will generate the SQL query
    :return: str: The generated SQL query.

    """
    # First we will clean the User Question.
    user_question = agent.team_session_state["application_response"].user_question
    asyncio.run(_publish_update_to_queue(agent)) # Publish update

    cleaned_question = user_question.replace("'", "").replace('"', '').replace("?", "").replace("!", "").strip()

    agent.team_session_state["application_response"].user_question = cleaned_question
    agent.team_session_state["application_response"].generated_sql_query = "Generating Embeddings for the User Question."
    agent.team_session_state["application_response"].explanation = "Generating Embeddings for the User Question."
    asyncio.run(_publish_update_to_queue(agent)) # Publish update

    # Then we will use the ChromaDB to retrieve the relevant tables and columns.
    client = chromadb.PersistentClient(path="Embeddings/", settings=Settings(anonymized_telemetry=False))
    table_collection = client.get_collection(name="table_names")
    tables_result = table_collection.query(
        query_texts=[cleaned_question],
        n_results=3
    )
    agent.team_session_state["application_response"].generated_sql_query = "Getting relevant Tables and Columns for the User Question."
    agent.team_session_state["application_response"].explanation = "Getting relevant Tables and Columns for the User Question."
    asyncio.run(_publish_update_to_queue(agent))  # Publish update

    # Then we will use the retrieved tables and columns to generate the SQL query.
    column_collection = client.get_collection(name="column_names")
    columns_result = column_collection.query(
        query_texts=[cleaned_question],
        n_results=5
    )

    agent.team_session_state["application_response"].generated_sql_query = "Getting relevant Examples for the User Question."
    agent.team_session_state[
        "application_response"].explanation = "Getting relevant Examples for the User Question."
    asyncio.run(_publish_update_to_queue(agent))  # Publish update

    example_collection = client.get_collection(name="examples")
    example_queries = example_collection.query(
        query_texts=[cleaned_question],
        n_results=3
    )

    db_type = "Postgres"

    sql_prompt = f""" You are an expert SQL query generator. Your task is to translate natural language questions into accurate and efficient {db_type}-compatible SQL queries.
    
    You will be provided with:
    1.  **User Question:** {user_question}
    2.  **Relevant Tables:** {','.join(tables_result["documents"][0])}
    3.  **Relevant Column Data :** { ' \n '.join([str(single_col) for single_col in columns_result["metadatas"][0]])}
    4.  **Database Schema:** dbo
    5.  **Example SQLs:** 
        {' \n '.join([str(single_example) for single_example in example_queries["metadatas"][0]])}
    ---
    
    **Instructions to follow:**
    
    * **Compatibility:** Ensure all generated SQL syntax, functions, and data types are compatible with {db_type}.
    * **Clarity and Readability:** Generate clear, well-formatted SQL queries.
    * **Accuracy:** The query must accurately address the user's question based on the provided information.
    * **Efficiency (where applicable):** Consider common query optimization techniques.
    * **Error Handling/Limitations:** If a query cannot be generated with the provided information, explain why.
    * **Common Operations:** Be prepared to handle `SELECT`, `WHERE`, `JOIN` (INNER, LEFT, RIGHT, FULL), `GROUP BY`, `ORDER BY`, `LIMIT`, `OFFSET`,
    aggregate functions (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`), subqueries, and common {db_type}-specific functions (e.g., `DATE_TRUNC`, `EXTRACT`, `COALESCE`).
    * **Bring Relevant Columns:** In the generated SQL query, include only the columns that are relevant to the User Question. Not all columns are required.
    
"""
    agent.team_session_state["application_response"].generated_sql_query = "Getting SQL based on the User Question."
    agent.team_session_state[
        "application_response"].explanation = "Getting SQL based on the User Question."
    asyncio.run(_publish_update_to_queue(agent))  # Publish update

    client = genai.Client(api_key=os.environ["GOOGLE_API_KEY"])
    llm_response = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=sql_prompt,
        config=types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(thinking_budget=-1),
            temperature=0.2,
            response_mime_type="application/json",
            response_schema=SQLOutput
        )
    )

    output_response: SQLOutput = llm_response.parsed
    agent.team_session_state["application_response"].generated_sql_query = output_response.generated_sql_query
    agent.team_session_state["application_response"].explanation = output_response.explanation
    asyncio.run(_publish_update_to_queue(agent)) # Publish update

    return output_response.generated_sql_query


def internal_execute_query(sql_query: str, params: tuple = None) -> tuple:
    """
    Executes a SQL query against a PostgresSQL database and returns the results.
    :param sql_query: str: The SQL query to execute.
    :param params: tuple: Optional parameters for the SQL query.
    :return: tuple: A tuple containing the headers and rows of the result set.
    """
    try:
        db_url = f"postgresql://{os.environ['POSTGRESQL_USERNAME']}:{os.environ['POSTGRESQL_PASSWORD']}@{os.environ['POSTGRESQL_HOST']}:{os.environ['POSTGRESQL_PORT']}/{os.environ['POSTGRESQL_DATABASE']}"
        with psycopg.connect(db_url) as conn:
            with conn.cursor() as cursor:
                cursor.execute(sql_query, params)
                if cursor.description:
                    headers = [desc[0] for desc in cursor.description]
                    rows = cursor.fetchall()
                    return headers, rows
                else:
                    return [], []
    except Exception as e:
        print(f"Error executing query: {e}")
        return [], []


def execute_query(agent: Agent, sql_query: str) -> tuple:
    """
    Executes a SQL query against a PostgresSQL database and returns the results.It does not generate the SQL query, it only executes it.
    :param agent: Agno Agent: The agent that will execute the SQL query.
    :param sql_query: str: The SQL query to execute.
    :return: tuple: A tuple containing the headers and rows of the result set.
    """
    try:
        db_url = f"postgresql://{os.environ['POSTGRESQL_USERNAME']}:{os.environ['POSTGRESQL_PASSWORD']}@{os.environ['POSTGRESQL_HOST']}:{os.environ['POSTGRESQL_PORT']}/{os.environ['POSTGRESQL_DATABASE']}"
        with psycopg.connect(db_url) as conn:
            with conn.cursor() as cursor:
                cursor.execute(sql_query)
                if cursor.description:
                    headers = [desc[0] for desc in cursor.description]
                    rows = cursor.fetchall()
                    df = pd.DataFrame(rows, columns=headers)
                    agent.team_session_state["application_response"].dataframe = df
                    asyncio.run(_publish_update_to_queue(agent)) # Publish update
                    return headers, rows
                else:
                    return [], []
    except Exception as e:
        print(f"Error executing query: {e}")
        asyncio.run(_publish_update_to_queue(agent)) # Publish update
        return ["Error"], [[f"Failed to execute query: {e}"]]


def get_all_tables() -> list:
    """
    Retrieves all table names from the PostgreSQL database.
    :return: list: A list of table names.
    """
    query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'dbo';"
    headers, rows = internal_execute_query(query)
    return [row[0] for row in rows] if rows else []


def get_all_columns(table_name: str) -> list:
    """
    Retrieves all column names and their data types for a given table in the PostgreSQL database.
    :param table_name: str: The name of the table.
    :return: list: A list of tuples (column_name, data_type).
    """
    query = "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = %s;"
    headers, rows = internal_execute_query(query, (table_name,))
    return [(row[0], row[1]) for row in rows] if rows else []


def refresh_db_schema() -> str:
    """
    Refreshes the database schema by retrieving all tables and their columns.
    :return: str: A message indicating the schema has been refreshed.
    """
    # Delete the existing schema file if it exists.
    try:
        if os.path.exists("database_schema.json"):
            os.remove("database_schema.json")

        tables = get_all_tables()
        schema = {}
        for table in tables:
            for single_col, data_type in get_all_columns(table):
                if table not in schema:
                    schema[table] = []
                schema[table].append({"column_name": single_col, "data_type": data_type, "column_description": ""})

        # Write the schema to a JSON file.
        schema_file_path = "database_schema.json"
        with open(schema_file_path, 'w') as schema_file:
            import json
            json.dump(schema, schema_file, indent=4)

        generate_embeddings()

        return "Database schema refreshed successfully."
    except Exception as e:
        print(f"Error refreshing database schema: {e}")
        return "Failed to refresh database schema."


def generate_embeddings():
    client = chromadb.PersistentClient(path="Embeddings/",
                                       settings=Settings(allow_reset=True, anonymized_telemetry=False))
    client.reset()  # Empty the database before adding new data.


    # Storing Examples in ChromaDB.
    with open("examples.json", "r") as file:
        examples = file.read()
        examples = json.loads(examples)

    example_collection = client.create_collection(name="examples")

    print("Storing examples in ChromaDB...")
    example_collection.add(
        documents=[example["example_question"] for example in examples],
        metadatas=[{"example_question":example["example_question"], "example_answer": example["example_answer"]} for example in examples],
        ids=[str(uuid.uuid4()) for _ in range(0, len(examples))]
    )
    print("Examples stored in ChromaDB.")

    with open("database_schema.json", "r") as file:
        schema = file.read()

    database_schema = json.loads(schema)

    # Storing Table Names in ChromaDB.

    collection = client.create_collection(name="table_names")
    print("Storing table names in ChromaDB...")

    collection.add(
        documents=list(database_schema.keys()),
        metadatas=[{"table_name": table_name} for table_name in list(database_schema.keys())],
        ids=[str(uuid.uuid4()) for _ in range(0, len(database_schema.keys()))]
    )
    print("Table names stored in ChromaDB.")

    # Storing Column Names in ChromaDB.
    print("Storing column names in ChromaDB...")
    collection = client.create_collection(name="column_names")
    for table_name, columns in database_schema.items():
        collection.add(
            documents=[column["column_name"] for column in columns],
            metadatas=[
                {"table_name": table_name, "column_name": column["column_name"], "data_type": column["data_type"]} for
                column in columns],
            ids=[str(uuid.uuid4()) for _ in range(0, len(columns))]
        )
    print(f"Column stored in ChromaDB.")


def generate_conversation_id() -> str:
    """
    Generates a unique conversation ID.
    """
    return str(uuid.uuid4())


def generate_insights(agent: Agent, question: str) -> str:
    """
    Generates insights based on the data in the dataframe. Make sure it runs only after the SQL query is executed.
    :param agent: Agno Agent: The agent that will generate the insights.
    :param question: str: The user's question to guide the insights generation.
    :return: str: The generated insights and observations.
    """
    dataframe = agent.team_session_state["application_response"].dataframe
    if dataframe.empty:
        return "No data available to generate insights."

    # Convert the DataFrame to a string representation for the LLM.
    df_str = dataframe.to_csv(index=False)

    insight_prompt = f"""You are a business analytics expert. Analyze the given dataframe and the asked User Question. 
    Try to answer the question based on the data provided, and provide any insights that you feel can help to user.
    If you cannot answer the question, just say "I cannot answer this question based on the data provided.".

                        User Question:
                        {question}

                        Dataframe:
                        {df_str}

                        Output:
                        """
    client = Groq(api_key=os.environ["GROQ_API_KEY"])
    response = client.chat.completions.create(
        messages=[
            {"role": "user",
             "content": insight_prompt}
        ],
        model="mistral-saba-24b"
    )
    content = response.choices[0].message.content
    agent.team_session_state["application_response"].insights = content
    asyncio.run(_publish_update_to_queue(agent)) # Publish update
    return content
